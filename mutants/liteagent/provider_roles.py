"""
Provider-specific role configurations for LiteAgent.

This module defines expected message role configurations for different model providers,
helping ensure compatibility across different LLM APIs.
"""

from typing import Dict, List, Set, Optional, Any
import copy

# Define supported roles for each provider
PROVIDER_ROLES = {
    "openai": {"system", "user", "assistant", "function", "tool"},
    "anthropic": {"system", "user", "assistant"},  # No function or tool roles
    "mistral": {"system", "user", "assistant", "tool"},  # No function role
    "deepseek": {"system", "user", "assistant", "tool"},  # No function role
    "groq": {"system", "user", "assistant", "function", "tool"},
    "ollama": {"system", "user", "assistant"}  # Ollama needs special handling for tools
}

# Define which providers require specific role sequences
PROVIDER_CONSTRAINTS = {
    "mistral": {
        "last_message_role": {"user", "tool"},  # Last message must be user or tool
        "strict_sequence": True,  # Enforces strict role alternation
        "system_position": "first_only"  # System message can only appear at the beginning
    },
    "deepseek": {
        "last_message_role": {"user", "tool"},  # Last message must be user or tool
        "strict_sequence": True,  # Enforces strict role alternation
        "system_position": "first_only"  # System message can only appear at the beginning
    },
    "ollama": {
        "last_message_role": {"user"},  # Last message must be user 
        "strict_sequence": True,  # Enforces strict role alternation
        "system_position": "first_only",  # System message can only appear at the beginning
        "embed_tool_calls": True  # Embed tool calls in user messages
    }
}
from inspect import signature as _mutmut_signature
from typing import Annotated
from typing import Callable
from typing import ClassVar


MutantDict = Annotated[dict[str, Callable], "Mutant"]


def _mutmut_trampoline(orig, mutants, call_args, call_kwargs, self_arg = None):
    """Forward call to original or mutated function, depending on the environment"""
    import os
    mutant_under_test = os.environ['MUTANT_UNDER_TEST']
    if mutant_under_test == 'fail':
        from mutmut.__main__ import MutmutProgrammaticFailException
        raise MutmutProgrammaticFailException('Failed programmatically')      
    elif mutant_under_test == 'stats':
        from mutmut.__main__ import record_trampoline_hit
        record_trampoline_hit(orig.__module__ + '.' + orig.__name__)
        result = orig(*call_args, **call_kwargs)
        return result
    prefix = orig.__module__ + '.' + orig.__name__ + '__mutmut_'
    if not mutant_under_test.startswith(prefix):
        result = orig(*call_args, **call_kwargs)
        return result
    mutant_name = mutant_under_test.rpartition('.')[-1]
    if self_arg:
        # call to a class method where self is not bound
        result = mutants[mutant_name](self_arg, *call_args, **call_kwargs)
    else:
        result = mutants[mutant_name](*call_args, **call_kwargs)
    return result

def x_process_messages_for_provider__mutmut_orig(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_1(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = None
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_2(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(None)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_3(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.copy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_4(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = None
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_5(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.upper()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_6(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "XX/XX" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_7(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" not in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_8(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = None
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_9(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split(None)[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_10(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("XX/XX")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_11(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[1]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_12(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_13(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = None
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_14(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "XXopenaiXX"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_15(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "OPENAI"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_16(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = None
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_17(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(None, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_18(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, None)
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_19(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_20(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, )
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_21(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["XXopenaiXX"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_22(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["OPENAI"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_23(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = None
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_24(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(None, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_25(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, None)
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_26(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get({})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_27(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, )
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_28(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider != "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_29(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "XXollamaXX":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_30(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "OLLAMA":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_31(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" or "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_32(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get(None) == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_33(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("XXroleXX") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_34(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("ROLE") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_35(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") != "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_36(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "XXsystemXX" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_37(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "SYSTEM" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_38(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "XXtoolsXX" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_39(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "TOOLS" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_40(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" not in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_41(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = None
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_42(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get(None, [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_43(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", None)
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_44(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get([])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_45(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", )
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_46(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("XXtoolsXX", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_47(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("TOOLS", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_48(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = None
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_49(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join(None)
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_50(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "XX\nXX".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_51(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get(None, 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_52(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', None)}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_53(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_54(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', )}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_55(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('XXnameXX', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_56(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('NAME', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_57(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'XXunknownXX')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_58(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'UNKNOWN')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_59(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get(None, 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_60(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', None)}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_61(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_62(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', )}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_63(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('XXdescriptionXX', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_64(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('DESCRIPTION', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_65(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'XXNo descriptionXX')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_66(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'no description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_67(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'NO DESCRIPTION')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_68(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = None
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_69(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get(None, "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_70(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", None)
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_71(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_72(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", )
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_73(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("XXcontentXX", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_74(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("CONTENT", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_75(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "XXXX")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_76(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = None
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_77(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = None
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_78(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["XXcontentXX"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_79(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["CONTENT"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_80(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = None
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_81(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = None
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_82(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get(None) == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_83(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("XXsystem_positionXX") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_84(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("SYSTEM_POSITION") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_85(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") != "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_86(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "XXfirst_onlyXX"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_87(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "FIRST_ONLY"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_88(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = None
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_89(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = True
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_90(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = ""
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_91(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = None
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_92(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get(None, "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_93(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", None)
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_94(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_95(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", )
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_96(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("XXroleXX", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_97(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("ROLE", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_98(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "XXXX")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_99(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role != "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_100(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "XXassistantXX":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_101(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "ASSISTANT":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_102(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = None
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_103(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = False
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_104(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" or first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_105(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role != "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_106(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "XXsystemXX" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_107(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "SYSTEM" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_108(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is not None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_109(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = None
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_110(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = None
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_111(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get(None, "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_112(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", None)
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_113(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_114(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", )
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_115(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("XXroleXX", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_116(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("ROLE", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_117(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "XXXX")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_118(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role != "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_119(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "XXsystemXX":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_120(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "SYSTEM":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_121(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg != first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_122(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(None)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_123(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append(None)
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_124(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "XXroleXX": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_125(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "ROLE": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_126(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "XXuserXX",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_127(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "USER",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_128(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "XXcontentXX": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_129(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "CONTENT": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_130(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get(None, '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_131(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', None)}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_132(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_133(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', )}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_134(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('XXcontentXX', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_135(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('CONTENT', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_136(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', 'XXXX')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_137(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(None)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_138(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(None)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_139(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            break
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_140(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") or (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_141(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get(None) and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_142(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("XXembed_tool_callsXX") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_143(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("EMBED_TOOL_CALLS") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_144(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" and role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_145(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role != "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_146(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "XXfunctionXX" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_147(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "FUNCTION" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_148(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role != "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_149(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "XXtoolXX"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_150(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "TOOL"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_151(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = None
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_152(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get(None, "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_153(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", None)
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_154(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_155(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", )
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_156(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("XXnameXX", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_157(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("NAME", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_158(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "XXunknownXX")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_159(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "UNKNOWN")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_160(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = None
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_161(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get(None, "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_162(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", None)
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_163(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_164(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", )
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_165(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("XXcontentXX", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_166(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("CONTENT", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_167(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "XXXX")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_168(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append(None)
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_169(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "XXroleXX": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_170(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "ROLE": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_171(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "XXuserXX",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_172(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "USER",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_173(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "XXcontentXX": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_174(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "CONTENT": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_175(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            break
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_176(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_177(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" and (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_178(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role != "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_179(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "XXfunctionXX" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_180(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "FUNCTION" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_181(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" or normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_182(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role != "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_183(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "XXtoolXX" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_184(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "TOOL" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_185(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider != "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_186(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "XXanthropicXX"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_187(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "ANTHROPIC"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_188(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append(None)
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_189(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "XXroleXX": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_190(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "ROLE": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_191(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "XXassistantXX",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_192(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "ASSISTANT",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_193(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "XXcontentXX": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_194(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "CONTENT": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_195(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get(None, 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_196(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', None)} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_197(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_198(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', )} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_199(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('XXnameXX', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_200(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('NAME', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_201(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'XXunknownXX')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_202(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'UNKNOWN')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_203(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get(None, '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_204(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', None)}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_205(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_206(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', )}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_207(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('XXcontentXX', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_208(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('CONTENT', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_209(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', 'XXXX')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_210(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append(None)
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_211(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "XXroleXX": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_212(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "ROLE": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_213(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "XXuserXX",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_214(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "USER",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_215(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "XXcontentXX": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_216(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "CONTENT": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_217(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get(None, "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_218(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", None)
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_219(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_220(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", )
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_221(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("XXcontentXX", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_222(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("CONTENT", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_223(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "XXXX")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_224(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(None)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_225(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) or processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_226(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get(None, False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_227(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", None) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_228(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get(False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_229(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", ) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_230(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("XXstrict_sequenceXX", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_231(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("STRICT_SEQUENCE", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_232(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", True) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_233(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider not in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_234(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["XXmistralXX", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_235(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["MISTRAL", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_236(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "XXdeepseekXX", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_237(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "DEEPSEEK", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_238(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "XXollamaXX"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_239(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "OLLAMA"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_240(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = None
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_241(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = ""
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_242(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(None):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_243(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = None
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_244(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get(None, "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_245(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", None)
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_246(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_247(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", )
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_248(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("XXroleXX", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_249(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("ROLE", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_250(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "XXXX")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_251(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is not None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_252(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(None)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_253(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = None
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_254(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    break
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_255(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role or role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_256(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role != prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_257(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role == "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_258(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "XXsystemXX":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_259(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "SYSTEM":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_260(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role != "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_261(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "XXuserXX":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_262(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "USER":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_263(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append(None)
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_264(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"XXroleXX": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_265(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"ROLE": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_266(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "XXassistantXX", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_267(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "ASSISTANT", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_268(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "XXcontentXX": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_269(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "CONTENT": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_270(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "XXI understand.XX"})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_271(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "i understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_272(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I UNDERSTAND."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_273(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(None)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_274(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = None
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_275(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role != "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_276(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "XXassistantXX":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_277(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "ASSISTANT":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_278(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append(None)
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_279(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"XXroleXX": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_280(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"ROLE": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_281(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "XXuserXX", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_282(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "USER", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_283(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "XXcontentXX": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_284(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "CONTENT": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_285(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "XXPlease continue.XX"})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_286(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_287(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "PLEASE CONTINUE."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_288(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(None)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_289(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = None
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_290(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(None)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_291(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = None
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_292(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(None)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_293(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = None
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_294(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = None
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_295(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints or processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_296(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "XXlast_message_roleXX" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_297(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "LAST_MESSAGE_ROLE" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_298(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" not in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_299(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = None
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_300(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["XXlast_message_roleXX"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_301(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["LAST_MESSAGE_ROLE"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_302(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = None
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_303(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get(None)
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_304(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[+1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_305(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-2].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_306(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("XXroleXX")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_307(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("ROLE")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_308(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_309(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append(None)
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_310(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"XXroleXX": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_311(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"ROLE": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_312(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "XXuserXX", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_313(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "USER", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_314(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "XXcontentXX": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_315(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "CONTENT": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_316(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "XXContinue.XX"})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_317(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_318(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "CONTINUE."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_319(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint or seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_320(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = None
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_321(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = True
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_322(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = None
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_323(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = None
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_324(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get(None, "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_325(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", None)
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_326(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_327(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", )
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_328(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("XXroleXX", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_329(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("ROLE", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_330(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "XXXX")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_331(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role != "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_332(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "XXassistantXX":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_333(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "ASSISTANT":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_334(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = None
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_335(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = False
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_336(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(None)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_337(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" or seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_338(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role != "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_339(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "XXsystemXX" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_340(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "SYSTEM" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_341(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append(None)
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_342(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "XXroleXX": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_343(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "ROLE": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_344(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "XXuserXX",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_345(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "USER",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_346(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "XXcontentXX": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_347(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "CONTENT": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_348(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get(None, '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_349(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', None)}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_350(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_351(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', )}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_352(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('XXcontentXX', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_353(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('CONTENT', '')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_354(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', 'XXXX')}"
                })
            else:
                final_messages.append(msg)
        
        return final_messages
    
    return processed_messages 

def x_process_messages_for_provider__mutmut_355(messages: List[Dict], provider: str) -> List[Dict]:
    """
    Process messages to ensure compatibility with specific model providers.
    
    Args:
        messages: List of message dictionaries
        provider: Provider name (e.g., "openai", "anthropic", "mistral")
        
    Returns:
        Processed list of messages compatible with the provider
    """
    # Make a deep copy of the messages to avoid modifying the original list
    messages = copy.deepcopy(messages)
    
    normalized_provider = provider.lower()
    
    # Extract the base provider (e.g., "openai" from "openai/gpt-4")
    if "/" in normalized_provider:
        normalized_provider = normalized_provider.split("/")[0]
    
    # Use OpenAI as default if provider not found
    if normalized_provider not in PROVIDER_ROLES:
        normalized_provider = "openai"
    
    # Get the supported roles for this provider
    supported_roles = PROVIDER_ROLES.get(normalized_provider, PROVIDER_ROLES["openai"])
    constraints = PROVIDER_CONSTRAINTS.get(normalized_provider, {})
    
    # Handle Ollama system prompt with tools
    if normalized_provider == "ollama":
        for msg in messages:
            if msg.get("role") == "system" and "tools" in msg:
                tools = msg.get("tools", [])
                if tools:  # Only augment if there are actual tools
                    tools_description = "\n".join([
                        f"- {tool.get('name', 'unknown')}: {tool.get('description', 'No description')}"
                        for tool in tools
                    ])
                    current_content = msg.get("content", "")
                    augmented_content = (
                        f"{current_content}\n\n"
                        f"You have access to the following tools:\n{tools_description}\n\n"
                        f"To use a tool, clearly indicate the tool name and parameters like this:\n"
                        f"'I'll use the [TOOL_NAME] tool with parameters: parameter1=value1, parameter2=value2'\n"
                        f"After seeing a tool result, process it and continue helping the user."
                    )
                    msg["content"] = augmented_content
    
    # Simple processing for all messages
    processed_messages = []
    
    # Flag to strictly enforce system message position constraints
    has_system_position_constraint = constraints.get("system_position") == "first_only"
    
    # Initialize tracking variables
    seen_assistant = False
    first_system_message = None
    
    # First pass: find the first system message and check if we've seen any assistant messages
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "assistant":
            seen_assistant = True
        
        if role == "system" and first_system_message is None:
            first_system_message = msg
    
    # Second pass: Process messages with knowledge from first pass
    for msg in messages:
        role = msg.get("role", "")
        
        # Special handling for system messages
        if role == "system":
            # For providers with strict system position constraints (like Mistral)
            if has_system_position_constraint:
                if msg == first_system_message:
                    # Always preserve the first system message
                    processed_messages.append(msg)
                elif seen_assistant:
                    # Convert any system message after assistant to user message
                    processed_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg.get('content', '')}"
                    })
                else:
                    # System messages before any assistant can stay as system
                    processed_messages.append(msg)
            else:
                # For providers without strict system constraints
                processed_messages.append(msg)
            continue
        
        # Special handling for Ollama - embed tool calls and responses in user/assistant messages
        if constraints.get("embed_tool_calls") and (role == "function" or role == "tool"):
            tool_name = msg.get("name", "unknown")
            tool_content = msg.get("content", "")
            
            # Format the tool response as a user message for Ollama
            processed_messages.append({
                "role": "user",
                "content": f"Tool result from {tool_name}: {tool_content}\nPlease process this result and continue."
            })
            continue
        
        # Handle unsupported roles
        if role not in supported_roles:
            if role == "function" or (role == "tool" and normalized_provider == "anthropic"):
                # Convert function/tool message to assistant message
                processed_messages.append({
                    "role": "assistant",
                    "content": f"Function {msg.get('name', 'unknown')} returned: {msg.get('content', '')}"
                })
            else:
                # For other unsupported roles, convert to user message
                processed_messages.append({
                    "role": "user",
                    "content": msg.get("content", "")
                })
        else:
            # Role is supported, keep it as is
            processed_messages.append(msg)
    
    # Apply strict sequence constraints if needed
    if constraints.get("strict_sequence", False) and processed_messages:
        # Only apply strict sequence for models that require it
        if normalized_provider in ["mistral", "deepseek", "ollama"]:
            reorganized_messages = []
            prev_role = None
            
            for i, msg in enumerate(processed_messages):
                role = msg.get("role", "")
                
                # Handle first message (usually system)
                if prev_role is None:
                    reorganized_messages.append(msg)
                    prev_role = role
                    continue
                
                # For subsequent messages, ensure proper alternation
                if role == prev_role and role != "system":
                    # If same role appears consecutively (except system), force alternation
                    if role == "user":
                        # Add a placeholder assistant message
                        reorganized_messages.append({"role": "assistant", "content": "I understand."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    elif role == "assistant":
                        # Add a placeholder user message
                        reorganized_messages.append({"role": "user", "content": "Please continue."})
                        reorganized_messages.append(msg)
                        prev_role = role
                    else:
                        # For other consecutive roles, just append
                        reorganized_messages.append(msg)
                        prev_role = role
                else:
                    # Roles are different, no need for alternation
                    reorganized_messages.append(msg)
                    prev_role = role
            
            processed_messages = reorganized_messages
    
    # Check last message role constraint
    if "last_message_role" in constraints and processed_messages:
        allowed_last_roles = constraints["last_message_role"]
        current_last_role = processed_messages[-1].get("role")
        
        if current_last_role not in allowed_last_roles:
            # Add an empty user message to satisfy the constraint
            processed_messages.append({"role": "user", "content": "Continue."})
    
    # Final verification for system messages after assistant for strict providers
    if has_system_position_constraint and seen_assistant:
        # One more pass to ensure no system message sneaks through after an assistant
        seen_assistant_in_processed = False
        final_messages = []
        
        for msg in processed_messages:
            role = msg.get("role", "")
            
            if role == "assistant":
                seen_assistant_in_processed = True
                final_messages.append(msg)
            elif role == "system" and seen_assistant_in_processed:
                # Convert to user message if it somehow got through earlier checks
                final_messages.append({
                    "role": "user",
                    "content": f"System instruction: {msg.get('content', '')}"
                })
            else:
                final_messages.append(None)
        
        return final_messages
    
    return processed_messages 

x_process_messages_for_provider__mutmut_mutants : ClassVar[MutantDict] = {
'x_process_messages_for_provider__mutmut_1': x_process_messages_for_provider__mutmut_1, 
    'x_process_messages_for_provider__mutmut_2': x_process_messages_for_provider__mutmut_2, 
    'x_process_messages_for_provider__mutmut_3': x_process_messages_for_provider__mutmut_3, 
    'x_process_messages_for_provider__mutmut_4': x_process_messages_for_provider__mutmut_4, 
    'x_process_messages_for_provider__mutmut_5': x_process_messages_for_provider__mutmut_5, 
    'x_process_messages_for_provider__mutmut_6': x_process_messages_for_provider__mutmut_6, 
    'x_process_messages_for_provider__mutmut_7': x_process_messages_for_provider__mutmut_7, 
    'x_process_messages_for_provider__mutmut_8': x_process_messages_for_provider__mutmut_8, 
    'x_process_messages_for_provider__mutmut_9': x_process_messages_for_provider__mutmut_9, 
    'x_process_messages_for_provider__mutmut_10': x_process_messages_for_provider__mutmut_10, 
    'x_process_messages_for_provider__mutmut_11': x_process_messages_for_provider__mutmut_11, 
    'x_process_messages_for_provider__mutmut_12': x_process_messages_for_provider__mutmut_12, 
    'x_process_messages_for_provider__mutmut_13': x_process_messages_for_provider__mutmut_13, 
    'x_process_messages_for_provider__mutmut_14': x_process_messages_for_provider__mutmut_14, 
    'x_process_messages_for_provider__mutmut_15': x_process_messages_for_provider__mutmut_15, 
    'x_process_messages_for_provider__mutmut_16': x_process_messages_for_provider__mutmut_16, 
    'x_process_messages_for_provider__mutmut_17': x_process_messages_for_provider__mutmut_17, 
    'x_process_messages_for_provider__mutmut_18': x_process_messages_for_provider__mutmut_18, 
    'x_process_messages_for_provider__mutmut_19': x_process_messages_for_provider__mutmut_19, 
    'x_process_messages_for_provider__mutmut_20': x_process_messages_for_provider__mutmut_20, 
    'x_process_messages_for_provider__mutmut_21': x_process_messages_for_provider__mutmut_21, 
    'x_process_messages_for_provider__mutmut_22': x_process_messages_for_provider__mutmut_22, 
    'x_process_messages_for_provider__mutmut_23': x_process_messages_for_provider__mutmut_23, 
    'x_process_messages_for_provider__mutmut_24': x_process_messages_for_provider__mutmut_24, 
    'x_process_messages_for_provider__mutmut_25': x_process_messages_for_provider__mutmut_25, 
    'x_process_messages_for_provider__mutmut_26': x_process_messages_for_provider__mutmut_26, 
    'x_process_messages_for_provider__mutmut_27': x_process_messages_for_provider__mutmut_27, 
    'x_process_messages_for_provider__mutmut_28': x_process_messages_for_provider__mutmut_28, 
    'x_process_messages_for_provider__mutmut_29': x_process_messages_for_provider__mutmut_29, 
    'x_process_messages_for_provider__mutmut_30': x_process_messages_for_provider__mutmut_30, 
    'x_process_messages_for_provider__mutmut_31': x_process_messages_for_provider__mutmut_31, 
    'x_process_messages_for_provider__mutmut_32': x_process_messages_for_provider__mutmut_32, 
    'x_process_messages_for_provider__mutmut_33': x_process_messages_for_provider__mutmut_33, 
    'x_process_messages_for_provider__mutmut_34': x_process_messages_for_provider__mutmut_34, 
    'x_process_messages_for_provider__mutmut_35': x_process_messages_for_provider__mutmut_35, 
    'x_process_messages_for_provider__mutmut_36': x_process_messages_for_provider__mutmut_36, 
    'x_process_messages_for_provider__mutmut_37': x_process_messages_for_provider__mutmut_37, 
    'x_process_messages_for_provider__mutmut_38': x_process_messages_for_provider__mutmut_38, 
    'x_process_messages_for_provider__mutmut_39': x_process_messages_for_provider__mutmut_39, 
    'x_process_messages_for_provider__mutmut_40': x_process_messages_for_provider__mutmut_40, 
    'x_process_messages_for_provider__mutmut_41': x_process_messages_for_provider__mutmut_41, 
    'x_process_messages_for_provider__mutmut_42': x_process_messages_for_provider__mutmut_42, 
    'x_process_messages_for_provider__mutmut_43': x_process_messages_for_provider__mutmut_43, 
    'x_process_messages_for_provider__mutmut_44': x_process_messages_for_provider__mutmut_44, 
    'x_process_messages_for_provider__mutmut_45': x_process_messages_for_provider__mutmut_45, 
    'x_process_messages_for_provider__mutmut_46': x_process_messages_for_provider__mutmut_46, 
    'x_process_messages_for_provider__mutmut_47': x_process_messages_for_provider__mutmut_47, 
    'x_process_messages_for_provider__mutmut_48': x_process_messages_for_provider__mutmut_48, 
    'x_process_messages_for_provider__mutmut_49': x_process_messages_for_provider__mutmut_49, 
    'x_process_messages_for_provider__mutmut_50': x_process_messages_for_provider__mutmut_50, 
    'x_process_messages_for_provider__mutmut_51': x_process_messages_for_provider__mutmut_51, 
    'x_process_messages_for_provider__mutmut_52': x_process_messages_for_provider__mutmut_52, 
    'x_process_messages_for_provider__mutmut_53': x_process_messages_for_provider__mutmut_53, 
    'x_process_messages_for_provider__mutmut_54': x_process_messages_for_provider__mutmut_54, 
    'x_process_messages_for_provider__mutmut_55': x_process_messages_for_provider__mutmut_55, 
    'x_process_messages_for_provider__mutmut_56': x_process_messages_for_provider__mutmut_56, 
    'x_process_messages_for_provider__mutmut_57': x_process_messages_for_provider__mutmut_57, 
    'x_process_messages_for_provider__mutmut_58': x_process_messages_for_provider__mutmut_58, 
    'x_process_messages_for_provider__mutmut_59': x_process_messages_for_provider__mutmut_59, 
    'x_process_messages_for_provider__mutmut_60': x_process_messages_for_provider__mutmut_60, 
    'x_process_messages_for_provider__mutmut_61': x_process_messages_for_provider__mutmut_61, 
    'x_process_messages_for_provider__mutmut_62': x_process_messages_for_provider__mutmut_62, 
    'x_process_messages_for_provider__mutmut_63': x_process_messages_for_provider__mutmut_63, 
    'x_process_messages_for_provider__mutmut_64': x_process_messages_for_provider__mutmut_64, 
    'x_process_messages_for_provider__mutmut_65': x_process_messages_for_provider__mutmut_65, 
    'x_process_messages_for_provider__mutmut_66': x_process_messages_for_provider__mutmut_66, 
    'x_process_messages_for_provider__mutmut_67': x_process_messages_for_provider__mutmut_67, 
    'x_process_messages_for_provider__mutmut_68': x_process_messages_for_provider__mutmut_68, 
    'x_process_messages_for_provider__mutmut_69': x_process_messages_for_provider__mutmut_69, 
    'x_process_messages_for_provider__mutmut_70': x_process_messages_for_provider__mutmut_70, 
    'x_process_messages_for_provider__mutmut_71': x_process_messages_for_provider__mutmut_71, 
    'x_process_messages_for_provider__mutmut_72': x_process_messages_for_provider__mutmut_72, 
    'x_process_messages_for_provider__mutmut_73': x_process_messages_for_provider__mutmut_73, 
    'x_process_messages_for_provider__mutmut_74': x_process_messages_for_provider__mutmut_74, 
    'x_process_messages_for_provider__mutmut_75': x_process_messages_for_provider__mutmut_75, 
    'x_process_messages_for_provider__mutmut_76': x_process_messages_for_provider__mutmut_76, 
    'x_process_messages_for_provider__mutmut_77': x_process_messages_for_provider__mutmut_77, 
    'x_process_messages_for_provider__mutmut_78': x_process_messages_for_provider__mutmut_78, 
    'x_process_messages_for_provider__mutmut_79': x_process_messages_for_provider__mutmut_79, 
    'x_process_messages_for_provider__mutmut_80': x_process_messages_for_provider__mutmut_80, 
    'x_process_messages_for_provider__mutmut_81': x_process_messages_for_provider__mutmut_81, 
    'x_process_messages_for_provider__mutmut_82': x_process_messages_for_provider__mutmut_82, 
    'x_process_messages_for_provider__mutmut_83': x_process_messages_for_provider__mutmut_83, 
    'x_process_messages_for_provider__mutmut_84': x_process_messages_for_provider__mutmut_84, 
    'x_process_messages_for_provider__mutmut_85': x_process_messages_for_provider__mutmut_85, 
    'x_process_messages_for_provider__mutmut_86': x_process_messages_for_provider__mutmut_86, 
    'x_process_messages_for_provider__mutmut_87': x_process_messages_for_provider__mutmut_87, 
    'x_process_messages_for_provider__mutmut_88': x_process_messages_for_provider__mutmut_88, 
    'x_process_messages_for_provider__mutmut_89': x_process_messages_for_provider__mutmut_89, 
    'x_process_messages_for_provider__mutmut_90': x_process_messages_for_provider__mutmut_90, 
    'x_process_messages_for_provider__mutmut_91': x_process_messages_for_provider__mutmut_91, 
    'x_process_messages_for_provider__mutmut_92': x_process_messages_for_provider__mutmut_92, 
    'x_process_messages_for_provider__mutmut_93': x_process_messages_for_provider__mutmut_93, 
    'x_process_messages_for_provider__mutmut_94': x_process_messages_for_provider__mutmut_94, 
    'x_process_messages_for_provider__mutmut_95': x_process_messages_for_provider__mutmut_95, 
    'x_process_messages_for_provider__mutmut_96': x_process_messages_for_provider__mutmut_96, 
    'x_process_messages_for_provider__mutmut_97': x_process_messages_for_provider__mutmut_97, 
    'x_process_messages_for_provider__mutmut_98': x_process_messages_for_provider__mutmut_98, 
    'x_process_messages_for_provider__mutmut_99': x_process_messages_for_provider__mutmut_99, 
    'x_process_messages_for_provider__mutmut_100': x_process_messages_for_provider__mutmut_100, 
    'x_process_messages_for_provider__mutmut_101': x_process_messages_for_provider__mutmut_101, 
    'x_process_messages_for_provider__mutmut_102': x_process_messages_for_provider__mutmut_102, 
    'x_process_messages_for_provider__mutmut_103': x_process_messages_for_provider__mutmut_103, 
    'x_process_messages_for_provider__mutmut_104': x_process_messages_for_provider__mutmut_104, 
    'x_process_messages_for_provider__mutmut_105': x_process_messages_for_provider__mutmut_105, 
    'x_process_messages_for_provider__mutmut_106': x_process_messages_for_provider__mutmut_106, 
    'x_process_messages_for_provider__mutmut_107': x_process_messages_for_provider__mutmut_107, 
    'x_process_messages_for_provider__mutmut_108': x_process_messages_for_provider__mutmut_108, 
    'x_process_messages_for_provider__mutmut_109': x_process_messages_for_provider__mutmut_109, 
    'x_process_messages_for_provider__mutmut_110': x_process_messages_for_provider__mutmut_110, 
    'x_process_messages_for_provider__mutmut_111': x_process_messages_for_provider__mutmut_111, 
    'x_process_messages_for_provider__mutmut_112': x_process_messages_for_provider__mutmut_112, 
    'x_process_messages_for_provider__mutmut_113': x_process_messages_for_provider__mutmut_113, 
    'x_process_messages_for_provider__mutmut_114': x_process_messages_for_provider__mutmut_114, 
    'x_process_messages_for_provider__mutmut_115': x_process_messages_for_provider__mutmut_115, 
    'x_process_messages_for_provider__mutmut_116': x_process_messages_for_provider__mutmut_116, 
    'x_process_messages_for_provider__mutmut_117': x_process_messages_for_provider__mutmut_117, 
    'x_process_messages_for_provider__mutmut_118': x_process_messages_for_provider__mutmut_118, 
    'x_process_messages_for_provider__mutmut_119': x_process_messages_for_provider__mutmut_119, 
    'x_process_messages_for_provider__mutmut_120': x_process_messages_for_provider__mutmut_120, 
    'x_process_messages_for_provider__mutmut_121': x_process_messages_for_provider__mutmut_121, 
    'x_process_messages_for_provider__mutmut_122': x_process_messages_for_provider__mutmut_122, 
    'x_process_messages_for_provider__mutmut_123': x_process_messages_for_provider__mutmut_123, 
    'x_process_messages_for_provider__mutmut_124': x_process_messages_for_provider__mutmut_124, 
    'x_process_messages_for_provider__mutmut_125': x_process_messages_for_provider__mutmut_125, 
    'x_process_messages_for_provider__mutmut_126': x_process_messages_for_provider__mutmut_126, 
    'x_process_messages_for_provider__mutmut_127': x_process_messages_for_provider__mutmut_127, 
    'x_process_messages_for_provider__mutmut_128': x_process_messages_for_provider__mutmut_128, 
    'x_process_messages_for_provider__mutmut_129': x_process_messages_for_provider__mutmut_129, 
    'x_process_messages_for_provider__mutmut_130': x_process_messages_for_provider__mutmut_130, 
    'x_process_messages_for_provider__mutmut_131': x_process_messages_for_provider__mutmut_131, 
    'x_process_messages_for_provider__mutmut_132': x_process_messages_for_provider__mutmut_132, 
    'x_process_messages_for_provider__mutmut_133': x_process_messages_for_provider__mutmut_133, 
    'x_process_messages_for_provider__mutmut_134': x_process_messages_for_provider__mutmut_134, 
    'x_process_messages_for_provider__mutmut_135': x_process_messages_for_provider__mutmut_135, 
    'x_process_messages_for_provider__mutmut_136': x_process_messages_for_provider__mutmut_136, 
    'x_process_messages_for_provider__mutmut_137': x_process_messages_for_provider__mutmut_137, 
    'x_process_messages_for_provider__mutmut_138': x_process_messages_for_provider__mutmut_138, 
    'x_process_messages_for_provider__mutmut_139': x_process_messages_for_provider__mutmut_139, 
    'x_process_messages_for_provider__mutmut_140': x_process_messages_for_provider__mutmut_140, 
    'x_process_messages_for_provider__mutmut_141': x_process_messages_for_provider__mutmut_141, 
    'x_process_messages_for_provider__mutmut_142': x_process_messages_for_provider__mutmut_142, 
    'x_process_messages_for_provider__mutmut_143': x_process_messages_for_provider__mutmut_143, 
    'x_process_messages_for_provider__mutmut_144': x_process_messages_for_provider__mutmut_144, 
    'x_process_messages_for_provider__mutmut_145': x_process_messages_for_provider__mutmut_145, 
    'x_process_messages_for_provider__mutmut_146': x_process_messages_for_provider__mutmut_146, 
    'x_process_messages_for_provider__mutmut_147': x_process_messages_for_provider__mutmut_147, 
    'x_process_messages_for_provider__mutmut_148': x_process_messages_for_provider__mutmut_148, 
    'x_process_messages_for_provider__mutmut_149': x_process_messages_for_provider__mutmut_149, 
    'x_process_messages_for_provider__mutmut_150': x_process_messages_for_provider__mutmut_150, 
    'x_process_messages_for_provider__mutmut_151': x_process_messages_for_provider__mutmut_151, 
    'x_process_messages_for_provider__mutmut_152': x_process_messages_for_provider__mutmut_152, 
    'x_process_messages_for_provider__mutmut_153': x_process_messages_for_provider__mutmut_153, 
    'x_process_messages_for_provider__mutmut_154': x_process_messages_for_provider__mutmut_154, 
    'x_process_messages_for_provider__mutmut_155': x_process_messages_for_provider__mutmut_155, 
    'x_process_messages_for_provider__mutmut_156': x_process_messages_for_provider__mutmut_156, 
    'x_process_messages_for_provider__mutmut_157': x_process_messages_for_provider__mutmut_157, 
    'x_process_messages_for_provider__mutmut_158': x_process_messages_for_provider__mutmut_158, 
    'x_process_messages_for_provider__mutmut_159': x_process_messages_for_provider__mutmut_159, 
    'x_process_messages_for_provider__mutmut_160': x_process_messages_for_provider__mutmut_160, 
    'x_process_messages_for_provider__mutmut_161': x_process_messages_for_provider__mutmut_161, 
    'x_process_messages_for_provider__mutmut_162': x_process_messages_for_provider__mutmut_162, 
    'x_process_messages_for_provider__mutmut_163': x_process_messages_for_provider__mutmut_163, 
    'x_process_messages_for_provider__mutmut_164': x_process_messages_for_provider__mutmut_164, 
    'x_process_messages_for_provider__mutmut_165': x_process_messages_for_provider__mutmut_165, 
    'x_process_messages_for_provider__mutmut_166': x_process_messages_for_provider__mutmut_166, 
    'x_process_messages_for_provider__mutmut_167': x_process_messages_for_provider__mutmut_167, 
    'x_process_messages_for_provider__mutmut_168': x_process_messages_for_provider__mutmut_168, 
    'x_process_messages_for_provider__mutmut_169': x_process_messages_for_provider__mutmut_169, 
    'x_process_messages_for_provider__mutmut_170': x_process_messages_for_provider__mutmut_170, 
    'x_process_messages_for_provider__mutmut_171': x_process_messages_for_provider__mutmut_171, 
    'x_process_messages_for_provider__mutmut_172': x_process_messages_for_provider__mutmut_172, 
    'x_process_messages_for_provider__mutmut_173': x_process_messages_for_provider__mutmut_173, 
    'x_process_messages_for_provider__mutmut_174': x_process_messages_for_provider__mutmut_174, 
    'x_process_messages_for_provider__mutmut_175': x_process_messages_for_provider__mutmut_175, 
    'x_process_messages_for_provider__mutmut_176': x_process_messages_for_provider__mutmut_176, 
    'x_process_messages_for_provider__mutmut_177': x_process_messages_for_provider__mutmut_177, 
    'x_process_messages_for_provider__mutmut_178': x_process_messages_for_provider__mutmut_178, 
    'x_process_messages_for_provider__mutmut_179': x_process_messages_for_provider__mutmut_179, 
    'x_process_messages_for_provider__mutmut_180': x_process_messages_for_provider__mutmut_180, 
    'x_process_messages_for_provider__mutmut_181': x_process_messages_for_provider__mutmut_181, 
    'x_process_messages_for_provider__mutmut_182': x_process_messages_for_provider__mutmut_182, 
    'x_process_messages_for_provider__mutmut_183': x_process_messages_for_provider__mutmut_183, 
    'x_process_messages_for_provider__mutmut_184': x_process_messages_for_provider__mutmut_184, 
    'x_process_messages_for_provider__mutmut_185': x_process_messages_for_provider__mutmut_185, 
    'x_process_messages_for_provider__mutmut_186': x_process_messages_for_provider__mutmut_186, 
    'x_process_messages_for_provider__mutmut_187': x_process_messages_for_provider__mutmut_187, 
    'x_process_messages_for_provider__mutmut_188': x_process_messages_for_provider__mutmut_188, 
    'x_process_messages_for_provider__mutmut_189': x_process_messages_for_provider__mutmut_189, 
    'x_process_messages_for_provider__mutmut_190': x_process_messages_for_provider__mutmut_190, 
    'x_process_messages_for_provider__mutmut_191': x_process_messages_for_provider__mutmut_191, 
    'x_process_messages_for_provider__mutmut_192': x_process_messages_for_provider__mutmut_192, 
    'x_process_messages_for_provider__mutmut_193': x_process_messages_for_provider__mutmut_193, 
    'x_process_messages_for_provider__mutmut_194': x_process_messages_for_provider__mutmut_194, 
    'x_process_messages_for_provider__mutmut_195': x_process_messages_for_provider__mutmut_195, 
    'x_process_messages_for_provider__mutmut_196': x_process_messages_for_provider__mutmut_196, 
    'x_process_messages_for_provider__mutmut_197': x_process_messages_for_provider__mutmut_197, 
    'x_process_messages_for_provider__mutmut_198': x_process_messages_for_provider__mutmut_198, 
    'x_process_messages_for_provider__mutmut_199': x_process_messages_for_provider__mutmut_199, 
    'x_process_messages_for_provider__mutmut_200': x_process_messages_for_provider__mutmut_200, 
    'x_process_messages_for_provider__mutmut_201': x_process_messages_for_provider__mutmut_201, 
    'x_process_messages_for_provider__mutmut_202': x_process_messages_for_provider__mutmut_202, 
    'x_process_messages_for_provider__mutmut_203': x_process_messages_for_provider__mutmut_203, 
    'x_process_messages_for_provider__mutmut_204': x_process_messages_for_provider__mutmut_204, 
    'x_process_messages_for_provider__mutmut_205': x_process_messages_for_provider__mutmut_205, 
    'x_process_messages_for_provider__mutmut_206': x_process_messages_for_provider__mutmut_206, 
    'x_process_messages_for_provider__mutmut_207': x_process_messages_for_provider__mutmut_207, 
    'x_process_messages_for_provider__mutmut_208': x_process_messages_for_provider__mutmut_208, 
    'x_process_messages_for_provider__mutmut_209': x_process_messages_for_provider__mutmut_209, 
    'x_process_messages_for_provider__mutmut_210': x_process_messages_for_provider__mutmut_210, 
    'x_process_messages_for_provider__mutmut_211': x_process_messages_for_provider__mutmut_211, 
    'x_process_messages_for_provider__mutmut_212': x_process_messages_for_provider__mutmut_212, 
    'x_process_messages_for_provider__mutmut_213': x_process_messages_for_provider__mutmut_213, 
    'x_process_messages_for_provider__mutmut_214': x_process_messages_for_provider__mutmut_214, 
    'x_process_messages_for_provider__mutmut_215': x_process_messages_for_provider__mutmut_215, 
    'x_process_messages_for_provider__mutmut_216': x_process_messages_for_provider__mutmut_216, 
    'x_process_messages_for_provider__mutmut_217': x_process_messages_for_provider__mutmut_217, 
    'x_process_messages_for_provider__mutmut_218': x_process_messages_for_provider__mutmut_218, 
    'x_process_messages_for_provider__mutmut_219': x_process_messages_for_provider__mutmut_219, 
    'x_process_messages_for_provider__mutmut_220': x_process_messages_for_provider__mutmut_220, 
    'x_process_messages_for_provider__mutmut_221': x_process_messages_for_provider__mutmut_221, 
    'x_process_messages_for_provider__mutmut_222': x_process_messages_for_provider__mutmut_222, 
    'x_process_messages_for_provider__mutmut_223': x_process_messages_for_provider__mutmut_223, 
    'x_process_messages_for_provider__mutmut_224': x_process_messages_for_provider__mutmut_224, 
    'x_process_messages_for_provider__mutmut_225': x_process_messages_for_provider__mutmut_225, 
    'x_process_messages_for_provider__mutmut_226': x_process_messages_for_provider__mutmut_226, 
    'x_process_messages_for_provider__mutmut_227': x_process_messages_for_provider__mutmut_227, 
    'x_process_messages_for_provider__mutmut_228': x_process_messages_for_provider__mutmut_228, 
    'x_process_messages_for_provider__mutmut_229': x_process_messages_for_provider__mutmut_229, 
    'x_process_messages_for_provider__mutmut_230': x_process_messages_for_provider__mutmut_230, 
    'x_process_messages_for_provider__mutmut_231': x_process_messages_for_provider__mutmut_231, 
    'x_process_messages_for_provider__mutmut_232': x_process_messages_for_provider__mutmut_232, 
    'x_process_messages_for_provider__mutmut_233': x_process_messages_for_provider__mutmut_233, 
    'x_process_messages_for_provider__mutmut_234': x_process_messages_for_provider__mutmut_234, 
    'x_process_messages_for_provider__mutmut_235': x_process_messages_for_provider__mutmut_235, 
    'x_process_messages_for_provider__mutmut_236': x_process_messages_for_provider__mutmut_236, 
    'x_process_messages_for_provider__mutmut_237': x_process_messages_for_provider__mutmut_237, 
    'x_process_messages_for_provider__mutmut_238': x_process_messages_for_provider__mutmut_238, 
    'x_process_messages_for_provider__mutmut_239': x_process_messages_for_provider__mutmut_239, 
    'x_process_messages_for_provider__mutmut_240': x_process_messages_for_provider__mutmut_240, 
    'x_process_messages_for_provider__mutmut_241': x_process_messages_for_provider__mutmut_241, 
    'x_process_messages_for_provider__mutmut_242': x_process_messages_for_provider__mutmut_242, 
    'x_process_messages_for_provider__mutmut_243': x_process_messages_for_provider__mutmut_243, 
    'x_process_messages_for_provider__mutmut_244': x_process_messages_for_provider__mutmut_244, 
    'x_process_messages_for_provider__mutmut_245': x_process_messages_for_provider__mutmut_245, 
    'x_process_messages_for_provider__mutmut_246': x_process_messages_for_provider__mutmut_246, 
    'x_process_messages_for_provider__mutmut_247': x_process_messages_for_provider__mutmut_247, 
    'x_process_messages_for_provider__mutmut_248': x_process_messages_for_provider__mutmut_248, 
    'x_process_messages_for_provider__mutmut_249': x_process_messages_for_provider__mutmut_249, 
    'x_process_messages_for_provider__mutmut_250': x_process_messages_for_provider__mutmut_250, 
    'x_process_messages_for_provider__mutmut_251': x_process_messages_for_provider__mutmut_251, 
    'x_process_messages_for_provider__mutmut_252': x_process_messages_for_provider__mutmut_252, 
    'x_process_messages_for_provider__mutmut_253': x_process_messages_for_provider__mutmut_253, 
    'x_process_messages_for_provider__mutmut_254': x_process_messages_for_provider__mutmut_254, 
    'x_process_messages_for_provider__mutmut_255': x_process_messages_for_provider__mutmut_255, 
    'x_process_messages_for_provider__mutmut_256': x_process_messages_for_provider__mutmut_256, 
    'x_process_messages_for_provider__mutmut_257': x_process_messages_for_provider__mutmut_257, 
    'x_process_messages_for_provider__mutmut_258': x_process_messages_for_provider__mutmut_258, 
    'x_process_messages_for_provider__mutmut_259': x_process_messages_for_provider__mutmut_259, 
    'x_process_messages_for_provider__mutmut_260': x_process_messages_for_provider__mutmut_260, 
    'x_process_messages_for_provider__mutmut_261': x_process_messages_for_provider__mutmut_261, 
    'x_process_messages_for_provider__mutmut_262': x_process_messages_for_provider__mutmut_262, 
    'x_process_messages_for_provider__mutmut_263': x_process_messages_for_provider__mutmut_263, 
    'x_process_messages_for_provider__mutmut_264': x_process_messages_for_provider__mutmut_264, 
    'x_process_messages_for_provider__mutmut_265': x_process_messages_for_provider__mutmut_265, 
    'x_process_messages_for_provider__mutmut_266': x_process_messages_for_provider__mutmut_266, 
    'x_process_messages_for_provider__mutmut_267': x_process_messages_for_provider__mutmut_267, 
    'x_process_messages_for_provider__mutmut_268': x_process_messages_for_provider__mutmut_268, 
    'x_process_messages_for_provider__mutmut_269': x_process_messages_for_provider__mutmut_269, 
    'x_process_messages_for_provider__mutmut_270': x_process_messages_for_provider__mutmut_270, 
    'x_process_messages_for_provider__mutmut_271': x_process_messages_for_provider__mutmut_271, 
    'x_process_messages_for_provider__mutmut_272': x_process_messages_for_provider__mutmut_272, 
    'x_process_messages_for_provider__mutmut_273': x_process_messages_for_provider__mutmut_273, 
    'x_process_messages_for_provider__mutmut_274': x_process_messages_for_provider__mutmut_274, 
    'x_process_messages_for_provider__mutmut_275': x_process_messages_for_provider__mutmut_275, 
    'x_process_messages_for_provider__mutmut_276': x_process_messages_for_provider__mutmut_276, 
    'x_process_messages_for_provider__mutmut_277': x_process_messages_for_provider__mutmut_277, 
    'x_process_messages_for_provider__mutmut_278': x_process_messages_for_provider__mutmut_278, 
    'x_process_messages_for_provider__mutmut_279': x_process_messages_for_provider__mutmut_279, 
    'x_process_messages_for_provider__mutmut_280': x_process_messages_for_provider__mutmut_280, 
    'x_process_messages_for_provider__mutmut_281': x_process_messages_for_provider__mutmut_281, 
    'x_process_messages_for_provider__mutmut_282': x_process_messages_for_provider__mutmut_282, 
    'x_process_messages_for_provider__mutmut_283': x_process_messages_for_provider__mutmut_283, 
    'x_process_messages_for_provider__mutmut_284': x_process_messages_for_provider__mutmut_284, 
    'x_process_messages_for_provider__mutmut_285': x_process_messages_for_provider__mutmut_285, 
    'x_process_messages_for_provider__mutmut_286': x_process_messages_for_provider__mutmut_286, 
    'x_process_messages_for_provider__mutmut_287': x_process_messages_for_provider__mutmut_287, 
    'x_process_messages_for_provider__mutmut_288': x_process_messages_for_provider__mutmut_288, 
    'x_process_messages_for_provider__mutmut_289': x_process_messages_for_provider__mutmut_289, 
    'x_process_messages_for_provider__mutmut_290': x_process_messages_for_provider__mutmut_290, 
    'x_process_messages_for_provider__mutmut_291': x_process_messages_for_provider__mutmut_291, 
    'x_process_messages_for_provider__mutmut_292': x_process_messages_for_provider__mutmut_292, 
    'x_process_messages_for_provider__mutmut_293': x_process_messages_for_provider__mutmut_293, 
    'x_process_messages_for_provider__mutmut_294': x_process_messages_for_provider__mutmut_294, 
    'x_process_messages_for_provider__mutmut_295': x_process_messages_for_provider__mutmut_295, 
    'x_process_messages_for_provider__mutmut_296': x_process_messages_for_provider__mutmut_296, 
    'x_process_messages_for_provider__mutmut_297': x_process_messages_for_provider__mutmut_297, 
    'x_process_messages_for_provider__mutmut_298': x_process_messages_for_provider__mutmut_298, 
    'x_process_messages_for_provider__mutmut_299': x_process_messages_for_provider__mutmut_299, 
    'x_process_messages_for_provider__mutmut_300': x_process_messages_for_provider__mutmut_300, 
    'x_process_messages_for_provider__mutmut_301': x_process_messages_for_provider__mutmut_301, 
    'x_process_messages_for_provider__mutmut_302': x_process_messages_for_provider__mutmut_302, 
    'x_process_messages_for_provider__mutmut_303': x_process_messages_for_provider__mutmut_303, 
    'x_process_messages_for_provider__mutmut_304': x_process_messages_for_provider__mutmut_304, 
    'x_process_messages_for_provider__mutmut_305': x_process_messages_for_provider__mutmut_305, 
    'x_process_messages_for_provider__mutmut_306': x_process_messages_for_provider__mutmut_306, 
    'x_process_messages_for_provider__mutmut_307': x_process_messages_for_provider__mutmut_307, 
    'x_process_messages_for_provider__mutmut_308': x_process_messages_for_provider__mutmut_308, 
    'x_process_messages_for_provider__mutmut_309': x_process_messages_for_provider__mutmut_309, 
    'x_process_messages_for_provider__mutmut_310': x_process_messages_for_provider__mutmut_310, 
    'x_process_messages_for_provider__mutmut_311': x_process_messages_for_provider__mutmut_311, 
    'x_process_messages_for_provider__mutmut_312': x_process_messages_for_provider__mutmut_312, 
    'x_process_messages_for_provider__mutmut_313': x_process_messages_for_provider__mutmut_313, 
    'x_process_messages_for_provider__mutmut_314': x_process_messages_for_provider__mutmut_314, 
    'x_process_messages_for_provider__mutmut_315': x_process_messages_for_provider__mutmut_315, 
    'x_process_messages_for_provider__mutmut_316': x_process_messages_for_provider__mutmut_316, 
    'x_process_messages_for_provider__mutmut_317': x_process_messages_for_provider__mutmut_317, 
    'x_process_messages_for_provider__mutmut_318': x_process_messages_for_provider__mutmut_318, 
    'x_process_messages_for_provider__mutmut_319': x_process_messages_for_provider__mutmut_319, 
    'x_process_messages_for_provider__mutmut_320': x_process_messages_for_provider__mutmut_320, 
    'x_process_messages_for_provider__mutmut_321': x_process_messages_for_provider__mutmut_321, 
    'x_process_messages_for_provider__mutmut_322': x_process_messages_for_provider__mutmut_322, 
    'x_process_messages_for_provider__mutmut_323': x_process_messages_for_provider__mutmut_323, 
    'x_process_messages_for_provider__mutmut_324': x_process_messages_for_provider__mutmut_324, 
    'x_process_messages_for_provider__mutmut_325': x_process_messages_for_provider__mutmut_325, 
    'x_process_messages_for_provider__mutmut_326': x_process_messages_for_provider__mutmut_326, 
    'x_process_messages_for_provider__mutmut_327': x_process_messages_for_provider__mutmut_327, 
    'x_process_messages_for_provider__mutmut_328': x_process_messages_for_provider__mutmut_328, 
    'x_process_messages_for_provider__mutmut_329': x_process_messages_for_provider__mutmut_329, 
    'x_process_messages_for_provider__mutmut_330': x_process_messages_for_provider__mutmut_330, 
    'x_process_messages_for_provider__mutmut_331': x_process_messages_for_provider__mutmut_331, 
    'x_process_messages_for_provider__mutmut_332': x_process_messages_for_provider__mutmut_332, 
    'x_process_messages_for_provider__mutmut_333': x_process_messages_for_provider__mutmut_333, 
    'x_process_messages_for_provider__mutmut_334': x_process_messages_for_provider__mutmut_334, 
    'x_process_messages_for_provider__mutmut_335': x_process_messages_for_provider__mutmut_335, 
    'x_process_messages_for_provider__mutmut_336': x_process_messages_for_provider__mutmut_336, 
    'x_process_messages_for_provider__mutmut_337': x_process_messages_for_provider__mutmut_337, 
    'x_process_messages_for_provider__mutmut_338': x_process_messages_for_provider__mutmut_338, 
    'x_process_messages_for_provider__mutmut_339': x_process_messages_for_provider__mutmut_339, 
    'x_process_messages_for_provider__mutmut_340': x_process_messages_for_provider__mutmut_340, 
    'x_process_messages_for_provider__mutmut_341': x_process_messages_for_provider__mutmut_341, 
    'x_process_messages_for_provider__mutmut_342': x_process_messages_for_provider__mutmut_342, 
    'x_process_messages_for_provider__mutmut_343': x_process_messages_for_provider__mutmut_343, 
    'x_process_messages_for_provider__mutmut_344': x_process_messages_for_provider__mutmut_344, 
    'x_process_messages_for_provider__mutmut_345': x_process_messages_for_provider__mutmut_345, 
    'x_process_messages_for_provider__mutmut_346': x_process_messages_for_provider__mutmut_346, 
    'x_process_messages_for_provider__mutmut_347': x_process_messages_for_provider__mutmut_347, 
    'x_process_messages_for_provider__mutmut_348': x_process_messages_for_provider__mutmut_348, 
    'x_process_messages_for_provider__mutmut_349': x_process_messages_for_provider__mutmut_349, 
    'x_process_messages_for_provider__mutmut_350': x_process_messages_for_provider__mutmut_350, 
    'x_process_messages_for_provider__mutmut_351': x_process_messages_for_provider__mutmut_351, 
    'x_process_messages_for_provider__mutmut_352': x_process_messages_for_provider__mutmut_352, 
    'x_process_messages_for_provider__mutmut_353': x_process_messages_for_provider__mutmut_353, 
    'x_process_messages_for_provider__mutmut_354': x_process_messages_for_provider__mutmut_354, 
    'x_process_messages_for_provider__mutmut_355': x_process_messages_for_provider__mutmut_355
}

def process_messages_for_provider(*args, **kwargs):
    result = _mutmut_trampoline(x_process_messages_for_provider__mutmut_orig, x_process_messages_for_provider__mutmut_mutants, args, kwargs)
    return result 

process_messages_for_provider.__signature__ = _mutmut_signature(x_process_messages_for_provider__mutmut_orig)
x_process_messages_for_provider__mutmut_orig.__name__ = 'x_process_messages_for_provider'